---
layout: single
title:  "Finding Shady Payments: How Not to Do It"
date:   2018-03-15 23:59:09 -0700
---

## Intro
Yesterday over in [/r/statistics](reddit.com/r/statistics) I got into a debate over [this blog post](https://medium.com/@whstancil/statistical-model-strongly-suggests-the-stormy-daniels-payoff-came-from-the-trump-campaign-7c09c300cb18) on work by Will Stancil and his brother.  Quick bit of background: back in 2016 Trump allegedly had a tryst with adult-film star Stormy Daniels, and then paid her $130,000 to sign an NDA.  While this is obviously slimy as hell, it's not illegal.  What is illegal, is that it's possible he may have [used campaign funds to pay the $130K](https://twitter.com/TheViewFromLL2/status/971538152367251457); there are 5 payments the Trump Org made starting just 10 days before the NDA was signed that add up to \$129,999.72.

Some data-oriented folks on Twitter decided to use statistics to assess whether this was likely to have occurred by chance, ie. whether the sum of these 5 payments hitting almost $130K is statistical noise.  Their methodology has been to compile a list of Trump Campaign payments made over some relevant time period (for our purposes let's say the month of October) and resample this list to collect $N$ sets of 5 payments.  They sum each set of 5 payments, and look at the resulting empirical distribution.  They found that when they did so, the occurence of summing to within a dollar of \$130K was fairly rare (about .1%).  They therefore concluded that it was probably not chance that these payments summed to \$130K, and hence it's likely the Trump campaign paid off Stormy Daniels.

I first want to make it clear that I'm 100% anti-Trump, and that my criticism of this approach is solely based on my desire to improve the quality of criticism directed at him.  In no way is this an attempt to exonerate him, and looking at their work, despite disagreeing with their methods I do think the timing of these payments is suspicious as hell.  My problem is that I don't see utility in framing this argument statistically, because "are these payments noise" is intrinsically hard to frame in a rigorous statistical manner.  Let me explain why...

##Hypothesis Testing
So the basic framework that Stancil et. al. are trying to use is effectively a variant of the classic Neyman-Pearson approach to hypothesis testing.  Their basic question is "how likely is it that five payments sum to $130K by chance?"  Their reasoning is that if they can meaningfully assign a probability to "summing to 130K by chance", and that probability is low, then they can conclude that there was fraudulent intent behind these payments.  Before we dive into their method, I think it's helpful to quickly review the basic N-P approach to these kinds of questions.

So typically these methods compare two generic hypotheses, the "null hypothesis" ($H_0$, that the data occurred "by chance")  and the "alternative hypothesis" ($H_a$, that the data did not occur by chance).  Let's make this concrete with a classic example: the Lady Tasting Tea.  The basic idea is that there is a Lady (in the sense of a British peerage) who claims that the can tell by taste if milk was added first to a cup of tea, before the water was poured in.  We want to test this with an experiment, so we prepare $N$ cups of tea, half of which will have milk added first, and the other half will have water added first.  The Lady will taste the cups of tea in a random order, and then afterward choose the $N/2$ cups of tea that she thinks had milk added first.  We then tally up the number of times that she is correct (call each correct guess a "win"), and attempt to determine if her winning percentage is different from $50/%$, random chance.

Let's say that the Lady's true winning percentage (her ability to distinguish if milk was added first) is $\theta$.  Our null hypothesis is therefore $H_0: \theta = .5$, so any wins that she gets are just "by chance", ie. statistical noise.  We want to compare this to the alternative hypothesis $H_a: \theta > .5$.  Now, our analysis of the data will produce one of two conclusions: the data leads us to reject the null hypothesis, or the data does not lead us to reject the null hypothesis.  Depending on whether or not $H_0$ is true, there are four possible outcomes of this experiment:
| | Do not reject $H_0$ | Reject $H_0$  |
| --- |---| ---|
| $H_0$ is true | We are correct | We are incorrect (false-positive) |
| $H_a$ is true | We are incorrect (false-negative) | We are correct |

The N-P approach to this problem is to compare the data that we observed to the sampling distribution we would expect if $H_0$ is true.  If the data is "unlikely" under the null-hypothesis then we have grounds to reject it.  By choosing an appropriate definition of "unlikely" we can actually control the probability that we produce a false-positive.  Say that the Lady guesses $w$ cups of tea correctly, the way that we can control the false-positive rate is by looking at the CDF (technically 1-CDF, but whatever) of correct guesses $W$ under the null hypothesis: $P[ W \geq w | \theta = .5 ]$.  We will reject the null hypothesis if $P[ W \geq w | \theta = .5 ] < \alpha$, where $\alpha$ is called the significance level.  This procedure has two useful features:
(1) When $\alpha$ is small then the data is "extreme" in the sense that it lies in the tails of the sampling distribution; it is unlikely to occur under $H_0$.
(2) If we have some random variable $X$, and we have a functional form for its CDF $F(x) = P[ X \geq x ]$, then if we draw random samples of $X$ and plug those samples into $F$, the distribution of $F(X)$ will be uniformly distributed from 0 to 1.  This means that if we reject the null hypothesis whenever $P[ W \geq w | \theta = .5 ] < \alpha$, then under the null hypothesis we expect the event $F(w) < \alpha$ to occur exactly $\alpha$-percent of the time.  In other words, the significance level is equal to the false-positive rate!

In my opinion the mistake of Stancil et. al. is that they are using a faulty rejection criteria.  In the Lady Tasting Tea example above, we rejected the null whenever $P[ W \geq w ] < \alpha$.  As far as I can tell Stancil is instead "rejecting the null" when $P[ |W - w| < \epsilon| \theta = .5] < \alpha$, ie. when the probability of getting data *close* to what we we observed is sufficiently small.  The problem here is that, while this procedure still kind of has feature (1) from above (we are rejecting the null when the data is unlikely to occur under the null distribution), it lacks feature (2).  This method will not control the false-positive rate in the way that we want because the distribution of $P[ |W - w| < \epsilon| \theta = .5]$ is not uniform over $0$ to $1$, and will therefore not occur $\alpha$-percent of the time.  Instead this event will occur $\epsilon$ percent of the time, let's see why.

First we do a little probability algebra:
\$\$
\begin{equation}
\begin{split}
P[ |W - w| < \epsilon| \theta = .5] &= P[ (W - w) < \epsilon | \theta = .5] + P[ (W - w) > -\epsilon) | \theta =.5]\\
&= P[ W < \epsilon + w | \theta = .5] + P[ W  > w -\epsilon) | \theta =.5]\\
&= 1 - P[ W > w + \epsilon | \theta = .5] + P[ W  > w -\epsilon) | \theta =.5]\\
&=  1 - (P[ W > w -\epsilon) | \theta =.5] - P[ W  > w +\epsilon) | \theta =.5] )
\end{split}
\end{equation}
$$
Now we note that each term in $$P[ W  > w - \epsilon) | \theta =.5] - P[ W  > w +\epsilon) | \theta =.5]$$ is a uniform random variable


##The Lady Misusing Campaign Funds

