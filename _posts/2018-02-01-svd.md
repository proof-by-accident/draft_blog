---
layout: single
title:  "Stray Observations on the SVD"
date:   2018-02-01 23:59:09 -0700
---

Probably the saddest day of 7th grade was when we covered matrices in Algebra.  I think I had only just seen "The Matrix" like 9 months prior, and as a result it was one of the most disappointing lectures I've ever sat through.  Now that I've been working with them for a few more years I definitely find the subject more interesting.  One fun thing that it took me a while to completely understand is the Singular Value Decomposition (SVD).  The SVD is one of those things that sits at an intersection of a lot of different fields.  Any "data" fields use it for PCA or pretty much any kind of dimensional reduction (which is how I've come to it), but it also is useful in any kind of numerical linear algebra which makes it useful in computational fields as well.  One thing that I've realized more recently is that the SVD can also be used to nicely motivate the construction of tensors.  Tensors have been a white whale for me since at least my 3rd year of undergrad, so finally starting to see the connections has been especially validating.\\

In this post I'm hoping to give a "linear algebra" construction of the SVD in a way that make a number of its popular properties more intuitively clear.  I think that this perspective offers a better view of things like PCA and Low-Rank Approximations, one that is harder to grasp from the "top-down" definition which is usually something like:
>For any $$ r \times c $$ matrix $$A$$ its Singular Value Decomposition is a factorization $$W D V$$ where $$D$$ is a $$k \times k$$ diagonal matrix, and $$W$$ and $$V$$ are unitary matrices of dimension $$r \times k$$ and $$c \times k$$ respectively.

I think this is a good definition, but I think it obscures a lot of what this really means.  There's another definition that I've seen kicking around that, and I think it does a better job at clueing us in as to what the SVD is:
>The Singular Value Decomposition of any matrix is given by $$M =  \displaystyle \sum\limits_{i=1}^n \sigma_i \vec{u}_i \vec{v}_i^T$$ where the $$\sigma_i$$ are scalars, and $$u_i$ and $$v_i$ are column vectors.  Note that the way we multiply vectors and matrices means that $$u_i v_i^T$$ is actually a matrix $$M^{(i)}$$ whose elements are just $$M^{(i)}_{jk} = u_j v_k$$.  We call a  matrix like $$M_i$$ "rank one", which means that the output space of $$M_i$$, $$\{y \in \mathbf{R}^n : y= M_i x, \text{ for all x in } \mathbf{R}^n \}$$, is actually one dimensional.  This sounds kind of weird, but let's see why.

Recall that, if we have two column vectors $$x$$ and $$y$$, then $$x^Ty = \displaystyle \sum\limits_{i=1}^n x_i y_i$$ is a real number.  In fact, this is actually the inner (or "dot") product of the vectors $$x$$ and $$y$$, $$\left< x,y \right> = x \cdot y = x^Ty$$.  We can use this insight to understand what $$M_i$$ does to vector:
\$\$
M_i x = u_i v_i^T x = \left< x, v_i \right> u_i
$$

So $$M_i$$ is the matrix which first evaluates the inner product between $$x$$ and $$v_i$$, and then rescales the length of vector $$u_i$$ by that value.  This means that $$M_i x$$ lies in the 1-dimensional subspace spanned by $$u_i$$, $$\text{Span}(u_i) = \{x \in \mathbf{R}^n: x = \alpha u_i\}$$


So for the purposes of creating a fun narrative let's start with a vector $$\vec{x} \in \mathbf{R}^n$$, a function $$f : \mathbf{R}^n \rightarrow \mathbf{R}$$.  To emphasize that this function sends elements of $$\mathbf{R}^n$$ to $$\mathbf{R}$$ we give it the special (and slightly ridiculous) name "functional".  Now let's also say that $$f$$ is ***linear***.  This means that we can "break down" the functional $$f$$ over a *linear combination* of vectors:
\$\$
\begin{equation}
f(\alpha \vec{x} + \beta \vec{y}) = \alpha f(\vec{x}) + \beta f(\vec{x}) \tag{1}\label{eq:lin_func_def}
\end{equation}
$$

Where $\alpha$ and $\beta$ are scalars, and a linear combination of vectors is anything of the form $$\alpha \vec{x} + \beta \vec{y}$$.  Note that if we have two linear functions, $$f(\vec{x})$$ and $$g(\vec{x})$$, then with any pair of scalars $$\alpha,\beta$$ we can make a third function $$h(\vec{x}) = \alpha f(\vec{x}) + \beta g(\vec{x})$$.  It's not hard to convince yourself that $$h(\vec{x})$$ is also a linear function, ie. that if (1) is true of $$f$$ and $$g$$, then it is also true of $$\alpha f(\vec{x}) + \beta g(\vec{x})$$.  Furthermore it's pretty clear that if $$g(\vec{x}) = 0$$ for all $$\vec{x}$$, then $$g(\vec{x})$$ counts as a linear function, and so $$f(\vec{x}) = f(\vec{x}) + g(\vec{x})$$.  Since the set of all linear functionals contains a zero element, and linear combinations of linear functionals are still linear functionals, then the set of linear functionals from $$\mathbf{R}^n$$ to $$\mathbf{R}$$ is **itself a new vector space**. 

This is such an important fact, that we give this vector space it's own name: the *dual space* of $$\mathbf{R}^n$$, denoted $$\mathbf{R}^{*n}$$.  What we are going to find out




###Dual Vector Spaces
This is going to assume that you're familiar with the concept of a "vector space", but let's recap quickly.  A vector space is pair of sets, one containing vectors $$\mathbf{x} \in X$$ and the other containing scalars $$ \alpha \in A$$.  If you've got a vector $$ x = [x_1, x_2, ..., x_n]^T$$ then you can multiply by a scalar to get another vector $$\alpha x = [\alpha x_1, \alpha x_2, ..., \alpha x_n]^T$$.  Further, if you've got two vectors, you can add them to get third vector $$x + y = [x_1 + y_1, x_2 + y_2, ..., x_n + y_n ]$$.  
